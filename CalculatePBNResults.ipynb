{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbdb5f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-30T15:20:42.535552Z",
     "start_time": "2021-12-30T15:20:37.126712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example notebook to create dataframes from pbn files. Compatible with jupyter and vscode.\n",
    "# Minimal documentation provided. Assumes user is familiar with github, jupyter/vscode notebook and python.\n",
    "\n",
    "# author: Robert Salita (research@aipolice.org)\n",
    "# 1. read a pbn file (local file).\n",
    "# 2. create a df of deals, par, double dummy, single dummy probabilities, expected values, best contract (max expected value contract).\n",
    "\n",
    "# install:\n",
    "# 1. git clone https://github.com/BSalita/Calculate_PBN_Results\n",
    "# 2. pip install -U -r requirements.txt\n",
    "\n",
    "# requirements (specific to this project):\n",
    "# pandas\n",
    "# endplay\n",
    "# pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from endplay.parsers import pbn\n",
    "from endplay.types import Deal, Contract, Denom, Player, Penalty, Vul\n",
    "from endplay.dds import par, calc_all_tables\n",
    "from endplay.dealer import generate_deals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "direction_order = [0,2,1,3] # NSEW order\n",
    "suit_order = [3,2,1,0,4] # SHDCN order?\n",
    "pbn_filename = 'DDS_Camrose24_1- BENCAM22 v WBridge5.pbn' # local filename\n",
    "sd_productions = 100 # number of random deals to generate for calculating single dummy probabilities. Use smaller number for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d665c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 0\n",
    "#pd.options.display.min_rows = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read local pbn file\n",
    "pbn_file = pathlib.Path(pbn_filename)\n",
    "with open(pbn_file, 'r') as f:\n",
    "    boards = pbn.load(f)\n",
    "len(boards), vars(boards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fea482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df from boards\n",
    "df = pd.DataFrame([vars(b) for b in boards])\n",
    "for col in df.columns:\n",
    "    print(col, df[col].dtype)\n",
    "    if df[col].dtype == 'object':\n",
    "        if isinstance(df[col][0], dict):\n",
    "            df = pd.concat([df,pd.DataFrame.from_records(df[col])],axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate double dummy and par\n",
    "deals = df['deal']\n",
    "batch_size = 40\n",
    "t_t = []\n",
    "tables = []\n",
    "b_ptr = 0\n",
    "for b in range(0,len(deals),batch_size):\n",
    "    batch_tables = calc_all_tables(deals[b:min(b+batch_size,len(deals))])\n",
    "    tables.extend(batch_tables)\n",
    "    batch_t_t = (tt._data.resTable for tt in batch_tables)\n",
    "    t_t.extend(batch_t_t)\n",
    "    b_ptr += b\n",
    "\n",
    "assert len(deals) == len(t_t) == len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed48bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a few hands and double dummy tables\n",
    "dd_tricks_rows = []\n",
    "max_display = 4\n",
    "for ii,(dd,sd,tt) in enumerate(zip(deals,t_t,tables)):\n",
    "    if ii < max_display:\n",
    "        print(f\"Deal: {ii+1}\")\n",
    "        dd.pprint()\n",
    "        print()\n",
    "        tt.pprint()\n",
    "        print(tuple(tuple(sd[suit][direction] for suit in suit_order) for direction in direction_order))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e10d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of par scores (double dummy).\n",
    "pars = [par(tt, b, Player.north) for tt, b in zip(tables, df['board_num'])]  # middle arg is board (if int) otherwise enum vul.\n",
    "par_scores_ns = [parlist.score for parlist in pars]\n",
    "par_scores_ew = [-score for score in par_scores_ns]\n",
    "par_contracts = [[str(contract.level)+'SHDCN'[int(contract.denom)]+contract.declarer.abbr+contract.penalty.abbr+' '+str(contract.result) for contract in parlist] for parlist in pars]\n",
    "par_df = pd.DataFrame({'Par_NS':par_scores_ns,'Par_EW':par_scores_ew,'Par_Contracts_Result':par_contracts})\n",
    "par_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of double dummy tricks per direction and suit.\n",
    "dd_tricks_rows = [[sd[suit][direction] for direction in direction_order for suit in suit_order] for sd in t_t]\n",
    "dd_tricks_df = pd.DataFrame(dd_tricks_rows,columns=['_'.join(['DD_Tricks',d,s]) for d in 'NSEW' for s in 'CDHSN'])\n",
    "dd_tricks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72686747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of double dummy scores per direction and suit.\n",
    "def Tricks_To_Score(sd):\n",
    "    return [Contract(level=level,denom=suit,declarer=direction,penalty=Penalty.passed if sd[suit][direction]-6-level>=0 else Penalty.doubled,result=sd[suit][direction]-6-level).score(0) for direction in direction_order for suit in suit_order for level in range(1,8)]\n",
    "\n",
    "direction_order = [0,2,1,3] # NSEW order\n",
    "suit_order = [3,2,1,0,4] # SHDCN order?\n",
    "dd_score_rows = [Tricks_To_Score(sd) for sd in t_t]\n",
    "dd_score_df = pd.DataFrame(dd_score_rows,columns=['_'.join(['DD_Score',str(l)+s,d]) for d in 'NSEW' for s in 'CDHSN' for l in range(1,8)])\n",
    "dd_score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to calculate single dummy probabilities.\n",
    "\n",
    "# todo: obsolete these constants?\n",
    "CDHS = 'CDHS' # string ordered by suit rank - low to high\n",
    "CDHSN = CDHS+'N' # string ordered by strain\n",
    "NSHDC = 'NSHDC' # order by highest score value. useful for idxmax(). coincidentally reverse of CDHSN.\n",
    "SHDC = 'SHDC' # Hands, PBN, board_record_string (brs) ordering\n",
    "NSEW = 'NSEW' # double dummy solver ordering\n",
    "NESW = 'NESW' # Hands and PBN order\n",
    "NWES = 'NWES' # board_record_string (brs) ordering\n",
    "SHDCN = 'SHDCN' # ordering used by dds\n",
    "\n",
    "# todo: could save a couple seconds by creating dict of deals\n",
    "def calc_double_dummy_deals(deals, batch_size=40):\n",
    "    t_t = []\n",
    "    tables = []\n",
    "    for b in range(0,len(deals),batch_size):\n",
    "        batch_tables = calc_all_tables(deals[b:min(b+batch_size,len(deals))])\n",
    "        tables.extend(batch_tables)\n",
    "        batch_t_t = (tt._data.resTable for tt in batch_tables)\n",
    "        t_t.extend(batch_t_t)\n",
    "    assert len(t_t) == len(tables)\n",
    "    return deals, t_t, tables\n",
    "    return df\n",
    "\n",
    "def constraints(deal):\n",
    "    return True\n",
    "\n",
    "def generate_single_dummy_deals(predeal_string, produce, env=dict(), max_attempts=1000000, seed=None, show_progress=True, strict=True, swapping=0):\n",
    "    \n",
    "    predeal = Deal(predeal_string)\n",
    "\n",
    "    deals_t = generate_deals(\n",
    "        constraints,\n",
    "        predeal=predeal,\n",
    "        swapping=swapping,\n",
    "        show_progress=show_progress,\n",
    "        produce=produce,\n",
    "        seed=seed,\n",
    "        max_attempts=max_attempts,\n",
    "        env=env,\n",
    "        strict=strict\n",
    "        )\n",
    "\n",
    "    deals = tuple(deals_t) # create a tuple before interop memory goes wonky\n",
    "    \n",
    "    return calc_double_dummy_deals(deals)\n",
    "\n",
    "def calculate_single_dummy_probabilities(deal, produce=100):\n",
    "\n",
    "    ns_ew_rows = {}\n",
    "    for ns_ew in ['NS','EW']:\n",
    "        s = deal[2:].split()\n",
    "        if ns_ew == 'NS':\n",
    "            s[1] = '...'\n",
    "            s[3] = '...'\n",
    "        else:\n",
    "            s[0] = '...'\n",
    "            s[2] = '...'\n",
    "        predeal_string = 'N:'+' '.join(s)\n",
    "        #print_to_log(f\"predeal:{predeal_string}\")\n",
    "\n",
    "        d_t, t_t, tables = generate_single_dummy_deals(predeal_string, produce, show_progress=False)\n",
    "\n",
    "        rows = []\n",
    "        max_display = 4 # pprint only the first n generated deals\n",
    "        direction_order = [0,2,1,3] # NSEW order\n",
    "        suit_order = [3,2,1,0,4] # SHDCN order?\n",
    "        for ii,(dd,sd,tt) in enumerate(zip(d_t,t_t,tables)):\n",
    "            # if ii < max_display:\n",
    "                # print_to_log(f\"Deal:{ii+1} Fixed:{ns_ew} Generated:{ii+1}/{produce}\")\n",
    "                # dd.pprint()\n",
    "                # print_to_log()\n",
    "                # tt.pprint()\n",
    "                # print_to_log()\n",
    "            nswe_flat_l = [sd[suit][direction] for direction in direction_order for suit in suit_order]\n",
    "            rows.append([dd.to_pbn()]+nswe_flat_l)\n",
    "\n",
    "        dd_df = pd.DataFrame(rows,columns=['Deal']+[d+s for d in NSEW for s in CDHSN])\n",
    "        for d in NSEW:\n",
    "            for s in SHDCN:\n",
    "                ns_ew_rows[(ns_ew,d,s)] = dd_df[d+s].value_counts(normalize=True).reindex(range(14), fill_value=0).tolist() # ['Fixed_Direction','Direction_Declarer','Suit']+['SD_Prob_Take_'+str(n) for n in range(14)]\n",
    "    \n",
    "    return ns_ew_rows\n",
    "\n",
    "\n",
    "def append_single_dummy_results(pbns,sd_cache_d,produce=100):\n",
    "\n",
    "    for pbn in pbns:\n",
    "        if pbn not in sd_cache_d:\n",
    "            sd_cache_d[pbn] = calculate_single_dummy_probabilities(pbn, produce) # all combinations of declarer pair direction, declarer direciton, suit, tricks taken\n",
    "    return sd_cache_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f53c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes 1000 seconds for 100 sd calcs, or 10 sd calcs per second.\n",
    "sd_cache_d = {}\n",
    "pbns = [str(pbn) for pbn in deals]\n",
    "for i,pbn in enumerate(pbns):\n",
    "    print(f\"{i} of {len(pbns)} boards. pbn:{pbn}\")\n",
    "    if pbn not in sd_cache_d:\n",
    "        sd_cache_d[pbn] = calculate_single_dummy_probabilities(pbn, sd_productions) # all combinations of declarer pair direction, declarer direciton, suit, tricks taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8470a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate single dummy trick taking probability distribution\n",
    "sd_probs_d = defaultdict(list)\n",
    "for pbn in pbns:\n",
    "    #d['PBN'].append(pbn)\n",
    "    v = sd_cache_d[pbn]\n",
    "    print(pbn,v)\n",
    "    for (pair_direction,declarer_direction,suit),tricks in v.items():\n",
    "        for i,t in enumerate(tricks):\n",
    "            sd_probs_d['_'.join(['Probs',pair_direction,declarer_direction,suit,str(i)])].append(t)\n",
    "print(sd_probs_d)\n",
    "sd_probs_df = pd.DataFrame(sd_probs_d)\n",
    "sd_probs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate dict of contract result scores\n",
    "sd_scores_d = {}\n",
    "for suit in suit_order:\n",
    "    for level in range(1,8): # contract level\n",
    "        for tricks in range(14):\n",
    "            result = tricks-6-level\n",
    "            sd_scores_d[(level,'SHDCN'[suit],tricks,False)] = Contract(level=level,denom=suit,declarer=0,penalty=Penalty.passed if result>=0 else Penalty.doubled,result=result).score(Vul.none)\n",
    "            sd_scores_d[(level,'SHDCN'[suit],tricks,True)] = Contract(level=level,denom=suit,declarer=0,penalty=Penalty.passed if result>=0 else Penalty.doubled,result=result).score(Vul.both)\n",
    "sd_scores_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e56db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create score dataframe from dict\n",
    "scores_d = defaultdict(list)\n",
    "for suit in 'SHDCN':\n",
    "    for level in range(1,8):\n",
    "        for i in range(14):\n",
    "            scores_d['_'.join(['Score',str(level)+suit])].append([sd_scores_d[(level,suit,i,False)],sd_scores_d[(level,suit,i,True)]])\n",
    "print(scores_d)\n",
    "sd_scores_df = pd.DataFrame(scores_d)\n",
    "sd_scores_df.index.name = 'Taken'\n",
    "sd_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of expected values (probability * score)\n",
    "exp_d = defaultdict(list)\n",
    "pbn_vul = zip(pbns,df['_vul'])\n",
    "for pbn,vul in pbn_vul:\n",
    "    #print(pbn,vul)\n",
    "    for (pair_direction,declarer_direction,suit),probs in sd_cache_d[pbn].items():\n",
    "        is_vul = vul == 1 or (declarer_direction in 'NS' and vul == 2) or (declarer_direction in 'EW' and vul == 3)\n",
    "        #print(pair_direction,declarer_direction,suit,probs,is_vul)\n",
    "        for level in range(1,8):\n",
    "            #print(scores_d['_'.join(['Score',str(level)+suit])][is_vul])\n",
    "            exp_d['_'.join(['Exp',pair_direction,declarer_direction,suit,str(level)])].append(sum([prob*score[is_vul] for prob,score in zip(probs,scores_d['_'.join(['Score',str(level)+suit])])]))\n",
    "        #print(exp_d)\n",
    "#print(exp_d)\n",
    "sd_exp_df = pd.DataFrame(exp_d)\n",
    "sd_exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cfeb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for the column name of the max expected value, the max expected value, the contract having the max expected value.\n",
    "def create_best_contracts(r):\n",
    "    exp_tuples = tuple([(v,k) for k,v in r.items()])\n",
    "    ex_tuples_sorted = sorted(exp_tuples,reverse=True)\n",
    "    best_contract_tuple = ex_tuples_sorted[0]\n",
    "    best_contract_split = best_contract_tuple[1].split('_')\n",
    "    best_contract = best_contract_split[4]+best_contract_split[3]+best_contract_split[2]\n",
    "    return [best_contract_tuple[1],best_contract_tuple[0],best_contract_tuple[0] if best_contract_tuple[1][-5] in ['N','S'] else -best_contract_tuple[0],best_contract]\n",
    "\n",
    "sd_best_contract_l = sd_exp_df.apply(create_best_contracts,axis='columns')\n",
    "sd_best_contract_df = pd.DataFrame(sd_best_contract_l.tolist(),columns=['Exp_Max_Col','Exp_Max','Exp_Max_NS','Best_Contract'])\n",
    "sd_best_contract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([df,par_df,dd_tricks_df,dd_score_df,sd_best_contract_df],axis='columns')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c7a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_contract_to_contract(r):\n",
    "    return str(r['_contract']).upper().replace('♠','S').replace('♥','H').replace('♦','D').replace('♣','C').replace('NT','N')\n",
    "\n",
    "def convert_contract_to_declarer(r):\n",
    "    declarer = pd.NA if r['Contract'] == 'PASS' else r['Contract'][2]\n",
    "    assert declarer is pd.NA or declarer in 'NSEW', f\"declarer:{declarer}\"\n",
    "    return declarer\n",
    "\n",
    "def declarer_to_declarer_name(r):\n",
    "    declarer_name = pd.NA if r['Declarer'] is pd.NA else r[r['Declarer']]\n",
    "    return declarer_name\n",
    "\n",
    "def convert_contract_to_result(r):\n",
    "    result = pd.NA if r['Contract'] == 'PASS' else 0 if r['Contract'][-1] in ['=','0'] else int(r['Contract'][-1]) if r['Contract'][-2] == '+' else -int(r['Contract'][-1])\n",
    "    assert result is pd.NA or result in range(-13,8), f\"result:{result}\"\n",
    "    return result\n",
    "\n",
    "def convert_contract_to_tricks(r):\n",
    "    tricks = pd.NA if r['Contract'] == 'PASS' else int(r['Contract'][0])+6+r['Result']\n",
    "    assert tricks is pd.NA or tricks in range(0,14), f\"tricks:{tricks}\"\n",
    "    return tricks\n",
    "\n",
    "def convert_contract_to_dd_tricks(r):\n",
    "    dd_tricks = 0 if r['Contract'] == 'PASS' else dd_tricks_df['_'.join(['DD_Tricks',r['Declarer'],r['Contract'][1]])].iloc[r.name]\n",
    "    assert dd_tricks in range(0,14), f\"dd_tricks:{dd_tricks}\"\n",
    "    return dd_tricks\n",
    "\n",
    "def convert_score_to_score(r):\n",
    "    score_split = r['_score'].split()\n",
    "    assert len(score_split) == 2, f\"score_split:{score_split}\"\n",
    "    assert score_split[0] in ['NS','EW'], f\"score_split:{score_split[0]}\"\n",
    "    assert score_split[1][0] == '-' or str.isdigit(score_split[1][0]), f\"score_split:{score_split[1]}\"\n",
    "    score_split_direction = score_split[0]\n",
    "    score_split_value = score_split[1]\n",
    "    score_value = -int(score_split_value) if score_split_value[0] == '-' else int(score_split_value)\n",
    "    return score_value if score_split_direction == 'NS' else -score_value\n",
    "\n",
    "cols = ['board_num','deal','Room','_contract','Score','_vul','Par_NS','Exp_Max_Col','Exp_Max','Exp_Max_NS','Best_Contract','North','East','South','West']\n",
    "augmented_df = merged_df[cols].copy()\n",
    "augmented_df.rename(columns={'North':'N','East':'E','South':'S','West':'W'},inplace=True)\n",
    "augmented_df['Contract'] = augmented_df.apply(convert_contract_to_contract,axis='columns').astype('string')\n",
    "augmented_df['Declarer'] = augmented_df.apply(convert_contract_to_declarer,axis='columns').astype('string')\n",
    "augmented_df['Declarer_Name'] = augmented_df.apply(declarer_to_declarer_name,axis='columns').astype('string')\n",
    "augmented_df['Result'] = augmented_df.apply(convert_contract_to_result,axis='columns').astype('Int16')\n",
    "augmented_df['Tricks'] = augmented_df.apply(convert_contract_to_tricks,axis='columns').astype('UInt8')\n",
    "augmented_df['DD_Tricks'] = augmented_df.apply(convert_contract_to_dd_tricks,axis='columns').astype('UInt8')\n",
    "augmented_df.rename(columns={'Score':'_score'},inplace=True)\n",
    "augmented_df['Score_NS'] = augmented_df.apply(convert_score_to_score,axis='columns').astype('int16')\n",
    "augmented_df['Par_Diff_NS'] = augmented_df['Score_NS']-augmented_df['Par_NS'].astype('int16')\n",
    "augmented_df['DD_Tricks_Diff'] = augmented_df['Tricks']-augmented_df['DD_Tricks'].astype('int8')\n",
    "augmented_df['Exp_Max_Diff_NS'] = augmented_df['Score_NS']-augmented_df['Exp_Max_NS'].astype('int16')\n",
    "augmented_df.drop(columns=['_contract','_score'],inplace=True)\n",
    "augmented_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453cfa9",
   "metadata": {},
   "source": [
    "Following cells contain WIP experiments with comparative statistics; BENCAM22 vs WBridge5, Open vs Closed rooms, Tricks vs DD, par diffs, expected max diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() over Par_Diff_NS for all, bencam22, wbridge5\n",
    "print('Describe North, BENCAM22, Par_Diff_NS:')\n",
    "print(augmented_df[augmented_df['N'].eq('BENCAM22')]['Par_Diff_NS'].describe())\n",
    "print('Describe North, WBridge5, Par_Diff_NS:')\n",
    "print(augmented_df[augmented_df['N'].eq('WBridge5')]['Par_Diff_NS'].describe())\n",
    "\n",
    "# sum over Par_Diff_NS for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = augmented_df['Par_Diff_NS'].sum(),augmented_df[augmented_df['N'].eq('BENCAM22')]['Par_Diff_NS'].sum(),augmented_df[augmented_df['N'].eq('WBridge5')]['Par_Diff_NS'].sum()\n",
    "print(f\"Sum of Par_Diff_NS: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")\n",
    "\n",
    "# frequency where par was exceeded for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = sum(augmented_df['Par_Diff_NS'].gt(0)),sum(augmented_df['N'].eq('BENCAM22')&augmented_df['Par_Diff_NS'].gt(0)),sum(augmented_df['N'].eq('WBridge5')&augmented_df['Par_Diff_NS'].gt(0))\n",
    "print(f\"Frequency where exceeding Par: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28479ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() over DD_Tricks_Diff for all, bencam22, wbridge5\n",
    "print('Describe Declarer, BENCAM22, DD_Tricks_Diff:')\n",
    "print(augmented_df[augmented_df['Declarer_Name'].eq('BENCAM22')]['DD_Tricks_Diff'].describe())\n",
    "print('Describe Declarer, WBridge5, DD_Tricks_Diff:')\n",
    "print(augmented_df[augmented_df['Declarer_Name'].eq('WBridge5')]['DD_Tricks_Diff'].describe())\n",
    "\n",
    "# sum over DD_Tricks_Diff for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = augmented_df['DD_Tricks_Diff'].sum(),augmented_df[augmented_df['Declarer_Name'].eq('BENCAM22')]['DD_Tricks_Diff'].sum(),augmented_df[augmented_df['Declarer_Name'].eq('WBridge5')]['DD_Tricks_Diff'].sum()\n",
    "print(f\"Sum of DD_Tricks_Diff: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")\n",
    "\n",
    "# frequency where Tricks > DD for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = sum(augmented_df['DD_Tricks_Diff'].notna() & augmented_df['DD_Tricks_Diff'].gt(0)),sum(augmented_df[augmented_df['Declarer_Name'].eq('BENCAM22')]['DD_Tricks_Diff'].gt(0)),sum(augmented_df[augmented_df['Declarer_Name'].eq('WBridge5')]['DD_Tricks_Diff'].gt(0))\n",
    "print(f\"Frequency where Tricks > DD: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")\n",
    "\n",
    "# frequency where Tricks < DD for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = sum(augmented_df['DD_Tricks_Diff'].notna() & augmented_df['DD_Tricks_Diff'].lt(0)),sum(augmented_df[augmented_df['Declarer_Name'].eq('BENCAM22')]['DD_Tricks_Diff'].lt(0)),sum(augmented_df[augmented_df['Declarer_Name'].eq('WBridge5')]['DD_Tricks_Diff'].lt(0))\n",
    "print(f\"Frequency where Tricks < DD: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() over Par_Diff_NS for all, open, closed\n",
    "print(augmented_df['Par_Diff_NS'].describe(),augmented_df[augmented_df['Room'].eq('Open')]['Par_Diff_NS'].describe(),augmented_df[augmented_df['Room'].eq('Closed')]['Par_Diff_NS'].describe())\n",
    "# sum over Par_Diff_NS for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = augmented_df['Par_Diff_NS'].sum(),augmented_df[augmented_df['Room'].eq('Open')]['Par_Diff_NS'].sum(),augmented_df[augmented_df['Room'].eq('Closed')]['Par_Diff_NS'].sum()\n",
    "print(f\"Sum of Par_Diff_NS: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")\n",
    "all, open, closed = sum(augmented_df['Par_Diff_NS'].gt(0)),sum(augmented_df['Room'].eq('Open')&augmented_df['Par_Diff_NS'].gt(0)),sum(augmented_df['Room'].eq('Closed')&augmented_df['Par_Diff_NS'].gt(0))\n",
    "print(f\"Frequency where exceeding Par: All:{all} Open:{open} Closed:{closed} Open-Closed:{open-closed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe() over Exp_Max_Diff_NS for all, open, closed\n",
    "print(augmented_df['Exp_Max_Diff_NS'].describe(),augmented_df[augmented_df['Room'].eq('Open')]['Exp_Max_Diff_NS'].describe(),augmented_df[augmented_df['Room'].eq('Closed')]['Exp_Max_Diff_NS'].describe())\n",
    "# sum over Exp_Max_Diff_NS for all, bencam22, wbridge5\n",
    "all, bencam22, wbridge5 = augmented_df['Exp_Max_Diff_NS'].sum(),augmented_df[augmented_df['Room'].eq('Open')]['Exp_Max_Diff_NS'].sum(),augmented_df[augmented_df['Room'].eq('Closed')]['Exp_Max_Diff_NS'].sum()\n",
    "print(f\"Sum of Exp_Max_Diff_NS: All:{all} BENCAM22:{bencam22} WBridge5:{wbridge5} BENCAM22-WBridge5:{bencam22-wbridge5}\")\n",
    "all, open, closed = sum(augmented_df['Exp_Max_Diff_NS'].gt(0)),sum(augmented_df['Room'].eq('Open')&augmented_df['Exp_Max_Diff_NS'].gt(0)),sum(augmented_df['Room'].eq('Closed')&augmented_df['Exp_Max_Diff_NS'].gt(0))\n",
    "print(f\"Frequency where exceeding Exp_Max_Diff_NS: All:{all} Open:{open} Closed:{closed} Open-Closed:{open-closed}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57940f833d5abb0c0c347e8f3b5621e2a4106c67dd8c346e4d74fd3dfad0a9b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
